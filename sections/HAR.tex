%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Human Activity Recognition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:HAR}

In this section we describe the implementation of the human acitivity
recognition ('HAR') component on the mobile device.  The algorithmic
foundation of this data mining task have been described in detail in
deliverable D2.2 in section 3.2 ``Mobile Sensing and activity
recognition''. We include a brief summary here (\ref{sec:har_method})
for the sake of completeness. Subsection \ref{sec:har_component}
contains the descriptions of the implementation. In subsection
\ref{sec:har_eval} we evaluate our method to the on two data sets and
compare it to the state of the art approaches.

Despite several attempts to port the component to other mobile
platforms we have not been able to do so, due to technical
difficulties which where hard to anticipate. As described in
deliverable D1.1 it was planned to implement this component using the
Appcelerator Titanium\footnote{\url{http://www.appcelerator.com/}}
framework. Moreover, several viability checks were presented. In the
first prototypes
\footnote{\url{https://github.com/HeinrichHartmann/LiveGovWP1/tree/master/mobile/TitaniumSensorCollector}}
it turned out, that it is not possible to collect sensor data while
running in the background with Titanium. This feature is clearly
required for an integrated activity recognition library which shall be
used in the field trials.

As stated in D1.1, we propose independent native implementation as
fallback solution in this case. So far we have a running
implementation for Android and Blackberry. An iPhone port is planned,
once the implementation is more stable.


\subsection{HAR Method Summary}\label{sec:har_method}

The process of activity recognition uses a pipeline of signal
processing and machine learning techniques. It consists of two phases:
the ``training phase'' and the ``integration phase''. 

\begin{figure}[htbp]
\centering
\subfigure[HAR Training Phase]{
\label{fig:har_overview}
\includegraphics[width=0.45 \textwidth]{img/har/classification_overview.png}
} \hspace{1cm}
\subfigure[HAR Integration Phase]{
\label{fig:integrated_har_overview}
\includegraphics[width=0.25 \textwidth]{img/har/integration_overview.png}
}
\caption{Human Activity Recognition Method Overview}
\label{fig:HAR_PHASES}
\end{figure}

In the training phase (cf. Figure \ref{fig:har_overview}) a group of
volunteers is asked to perform the targeted activities for a certain
amount of time, while recording sensor samples with the device in
their pocket.  The gained training data stored in a database and used
to train a classifier of the activities.

In the integration phase (cf. \ref{fig:integrated_har_overview}), the
trained classifier is embedded into the mobile device.

Both phases rely on the preprocessing steps of "windowing'' and
``feature generation''. The stream of incoming sensor data is divided into
time windows of fixed size (typically 1-10 sec.) and for each window
a set of features is computed. This features are filtering out
relevant information from the raw signal. Typical features include
mean values and standard deviations as well as frequency domain
features like Fourier modes.

Deliverable D2.2 contains detailed lists of all sensors and features
and data mining methods that are used in the literature as well as
discussions of quality of the recognition results.

\subsection{Component Description}\label{sec:har_component}

The Human Activity Recognition Component implements the two phases
described in Figure \ref{fig:HAR_PHASES}. The training phase is
executed offline on a server and uses the following processing
pipeline (cf. Figure \ref{fig:classification_architecture}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{img/har/classification_architecture.jpg}
\caption{HAR Training Pipeline}\label{fig:classification_architecture}
\end{figure}

\begin{enumerate}
  \item {\bf Annotated Sensor Data.} The training data is stored as
    ssf files in the file system. The annotations are represented as
    folder structure. In this way it becomes very easy to add new
    training data to the repository. Using the inspection web tool,
    one can select appropriate time slices and export the
    corresponding samples as ``.ssf'' file. The exported file is then
    moved to the folder corresponding to the recorded activity.
  \item {\bf Sample Producer.} The sample producer service reads the 
    sensor samples from the file system and pushes them onto a generic 
    sample processing pipeline. The interface is designed a way that
    resembles the incoming data stream on the mobile device.
  \item {\bf Window Producer.} The window producer takes as
    constructor parameters a window-length and a delay. Incoming
    sensor samples are grouped in windows of the given length and
    passed further down as a single object.
  \item {\bf Interpolation and Quality Check.}  To remove frequency
    disparity and ignore timeframes below a set sample frequency a
    quality check is in place. After the check, all windows get
    interpolated to a constant frequency of $50Hz$.
  \item {\bf Feature Calculation.} The classification calculates
    different features extracted from windows of raw sampling data. A
    complete list of implemented features is provided in Figure
    \ref{fig:har_features}.
  \item {\bf CSV-Persistor.} The calculated feature vectors are stored
    on the file system as CSV file. 
\end{enumerate}


\begin{figure}
\centering
\begin{tabular}{|r|l|p{10cm}|} \hline
Index & Name     & Description \\ \hline
0            & id       & User ID of the recorded samples \\ \hline
1            & tag      & Name of the recoded activity    \\ \hline
2            & xMean    & Mean values of the individual axes \\ 
3            & yMean    &             \\ 
4            & zMean    &             \\ \hline
5            & xVar     & Variance of the individual axes  \\ 
6            & yVar     &             \\ 
7            & zVar     &             \\ \hline
8            & s2Mean   & Mean value of the length of the acceleration
                          vector. \\ \hline
9            & s2Var    & Variance of the length of the acceleration
                          vector \\ \hline
10           & tilt     & Average tilting angle of the device towards
                          the vertical axes. \\ \hline
11           & energy   & Total energy of the recording.  \\ \hline
12           & kurtosis & Kurtosis measure of "peakedness" of the
                          length of the acceleration vector. \\ \hline
13-24        & S2Bins   & Histogram over the length of the
                          acceleration vector. \\ \hline
25-37        & FFTBins  & Historgram over the absolute values of the
                          Fourier Modes of the length of the
                          acceleration vector. \\ \hline
\end{tabular}
\caption{Features Vectors used for HAR classification.}
\label{fig:har_features}
\end{figure}

In the next step several machine-learning methods to train classifiers
on the stored feature vectors. The individual methods are explained in
sections \ref{sec:DectionTree} and \ref{sec:SVM}.  The trained
classifier is then exported as a java module. The following processing
pipeline is used to integrate the classifier on the mobile device
(cf. Figure \ref{fig:integrated_har}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{img/har/integration.jpg}
\caption{Integrated HAR Architecture}\label{fig:integrated_har}
\end{figure}

\begin{enumerate}
\item {\bf Dispatcher Thread.} The dispatcher thread is part of
  the sensor collection architecture described in Chapter
  \ref{chap:sc}. It emits all gathered samples in the ssf format.
\item {\bf HAR Pipeline.} The HAR pipeline reuses most parts described
  in Figure \ref{fig:classification_architecture}. The Sample Producer
  is replaced by the Dispatcher Thread as source for sensor data.
\item {\bf HAR Classifier.} The HAR classifier component warps
  exported java method and outputs the results of the classification
  as string objects. The current implementation uses a decision tree.
\item {\bf Intent Broadcaster.} The classification results are
  broadcasted as an intent for use in other parts of the system
  (e.g. GUI, storage).
\end{enumerate}


\clearpage
\subsection{Classifier Training}\label{sec:har_classifier_training}



\subsubsection*{Decision Tree based classifier}
\label{sec:DectionTree}

% The training and test set get were imported into {\it
%   Weka}\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}}.  Weka
% comes with build in machine learning algorithms to choose from. Here
% the J48 implementation of the C4.5 algorithm is used to create a
% decision tree out of the provided training set. The test set is used
% to validate the created tree and calculate the accuracy and confusion
% matrix. Since Weka allows to export the created tree as Java class, it
% is possible to integrate the output directly into the HAR Pipeline.

\todo{HH}{Write section explaining Decision Tree Classifier.}

\subsubsection*{SVM based classifier}
\label{sec:SVM}

A popular algorithm to train a binary classification model for mapping
features to activities is the Support Vector Machines (SVMs). SVMs are
known for their ability in smoothly generalizing and coping
efficiently with high-dimensionality pattern recognition
problems. They define a hypothesis space that includes all the
possible linear separations of the data
(Fig.~\ref{fig:svm_hypothesisSpace}) and they choose the one that
maximizes the margin between the two classes
(Fig.~\ref{fig:svm_margin}).

\begin{figure}[h]
\centering
  \includegraphics[width=0.5\textwidth]{img/svms/Svms_hypothesisSpace.pdf}
  \caption{Left: Hypothesis space including all linear separations. Right: The selected hypothesis maximizes the margin.}
  \label{fig:svm_hypothesisSpace}
\end{figure}


\begin{figure}[h]
\centering
  \includegraphics[width=0.5\textwidth]{img/svms/Svm_Maxmargin.pdf}
  \caption{Maximization of the margin}
  \label{fig:svm_margin}
\end{figure}

\noindent The hyperplane that optimally separates the positive and the
negative class (i.e. maximizes the margin) can be described by
$\mathbf{w\cdot x + b = 0}$, where $\mathbf{w}$ is normal to the
hyperplane and $\frac{b}{\parallel\mathbf{w}\parallel}$ is the
perpendicular distance from the hyperplane to the origin
(Fig.~\ref{fig:svm_wb}). The optimal hyperplane can be obtained by
solving the following Quadratic Programming optimization problem:

\begin{equation}\label{Eq:svmQP0}
  \min \frac{1}{2}\parallel\mathbf{w}\parallel \quad \text{s.t.} \quad y_i(\mathbf{w}\cdot x_i + b) -1 \ge 0 \quad \forall i
\end{equation}

In order to relax the constraints of Eq.~\ref{Eq:svmQP0} and allow for
some misclassified points, a slack variable $\xi_i, i=1,\ldots,L$ is
introduced which transforms Eq.~\ref{Eq:svmQP0} into:

\begin{equation}\label{Eq:svmQPxi}
  \min \frac{1}{2}\parallel\mathbf{w}\parallel + C\sum_{i=1}^{L}\xi_i \quad \text{s.t.} \quad y_i(\mathbf{w}\cdot x_i + b) -1 +\xi_i \ge 0 \quad \forall i
\end{equation}

\noindent where the parameter $C$ controls the trade-off between the
slack variable penalty and the size of the margin. In the testing
phase, in order to classify an unseen example $x_t$, its distance to
the hyperplane is calculated using the formula $\mathbf{w}\cdot x_t +
b$. This distance indicates the classifier's confidence that the
unseen example $x_t$ belongs to the examined class.

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svms_wb.pdf}
  \caption{The resulting hyperplane after training an SVM.}
  \label{fig:svm_wb}
\end{figure}

The previous consider linear separation of the data. However, this is
rarely the case for most real world scenarios and for this reason the
kernel trick has been introduced. Applying the kernel trick to the
cases where the classes are not linearly separable in the input
feature space, we manage to map the features to a higher dimension
(Fig.~\ref{fig:svm_highDim}) where they can be linearly separated. For
example, in the case of an RBF kernel the otherwise linear hyperplane
is transformed to a hypersphere (Fig.~\ref{fig:svm_rbf}). For any
kernel $K(x,y)$, the classification model can be represented by a
vector $\textbf{w}$ (i.e. the model parameters), a bias scalar $b$ and
the support vectors $\mathbf{SV}_j, j=1,\ldots,N_{SV}$. For an unseen
example $x_t$, a confidence score is extracted by computing its
distance to the hyperplane of the model:

\begin{equation}\label{Eq:SVMdecision}
  confidence = \mathbf{w}*\sum_{j=1}^{N_{SV}}{K(\mathbf{SV}_j,\mathbf{x}_t)}+b
\end{equation}

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svm_DimensionMap.pdf}
  \caption{The data are not separable in the input space by a linear hyperplane. Using the kernel function, the data are mapped into a higher dimensional space where they can be linearly separated.}
  \label{fig:svm_highDim}
\end{figure}

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svm_RBF.pdf}
  \caption{Non-linear separation of the data using the RBF kernel}
  \label{fig:svm_rbf}
\end{figure}


In our case, an SVM model is trained on the previously extracted
features in order to learn the properties that define the examined
activity. The models are trained using the one versus all (OVA)
technique, i.e. all positive examples of the specific activity versus
all negative examples (i.e. the examples of all other activities). The
distance of a vector from the hyperplane indicates our confidence that
during the analysed window, the user performs the examined
activity. High positive values of this score increase our confidence
that this window belongs to the positive class while high negative
values provide strong confidence that the performed activity is not
the examined one.

% The fact that each model can be represented by a single vector and a
% scalar and that the testing process is essentially a vector
% multiplication, renders SVMs the best solution for a real time
% classification framework. This allows for storing the information
% about all the models in the phone memory, while testing is
% computationally very efficient, making it possible for the image
% classification algorithm to run entirely on a mobile phone.



\subsection{Evaluation}\label{sec:har_eval}

We have evaluated our classifier on two different datasets.

The first dataset was gathered on the University Campus in Koblenz in
December 2013.  It contains a total of around $900K$ samples collected
by $10$ volunteers.  The volunteers were instructed to perform the
activities ''walking'', ''running'', ''stairs'' and ''cycling'' on
predefined routes on the university campus. 
% (cf. Figure \ref{fig:data_collection_handout}). 
The total time effort per volunteer was about 20-25minutes and a financial reward was offered as
an incentive. After the recording the samples have been inspected
using our inspection tool and the beginning and ending of the
activities were manually stripped in order to avoid noise from holding
the device in the hand.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=\textwidth]{img/har/data_collection_handout.png}
%   \caption{Instructions for data collection in German
%     language}\label{fig:data_collection_handout}
% \end{figure}

The other dataset was obtained from the {\it UCI Machine Learning
  Repository}\footnote{\url{http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones}}
and was gathered by Davide Anguita, et. al. \cite{Anguita} in 2012.
It contains around $700K$ collected by 30 volunteers.

The number of samples per activity of both datasets are summarized in
Figure \ref{fig:har_datasets}. Both datasets contain only
accelerometer samples, and have been preprocessed that have been
sampled at a fixed rate of $50Hz$.

\begin{figure}[h]
\centering
\begin{tabular}{|l|r|r|} \hline
Activity  & UCI Dataset & UKOB Dataset \\ \hline
sitting   & 113.728     & 80.951        \\
standing  & 121.984     & 320.737       \\
walking   & 110.208     & 292.024       \\
running   & 0           & 31.916        \\
cycling   & 0           & 436.106       \\
stairs    & 188.800     & 30.086        \\
lying     & 124.416     & 0            \\ \hline \hline
Totals    & 659.136     & 903.156       \\ \hline
\end{tabular}
\caption{Number of accelerometer samples by activity and dataset.}
\label{fig:har_datasets}
\end{figure}

Both datasets have been split into a training data-set and a test
data-set. The individual parts contain only full recordings of the
activities. No single recording is present in both parts of the
split. The volume is distributed in roughly $66/33$ ratio between
both parts.

We have three different parameters for the evaluation:
\begin{itemize}
\item {\bf Classifier.} Decision Tree (DT) or support vector machine (SVM)
\item {\bf Features.} Manually selected (MAN) or PCA based (PCA)
\item {\bf Dataset.} Self collected (UKOB) ore reference dataset (UCI)
\end{itemize}

We get the following evaluation results:
\begin{center}
\begin{tabular}{|lll|r|} \hline
  {\bf Classifier} & {\bf Features} & {\bf Dataset} & {\bf Accuracy in \%} \\ \hline
  DT	& MAN	& UKOB	&	$62$ \\ 
  DT	& PCA	& UKOB	&	$45$\\ 
  SVM	& MAN	& UKOB	&	$87$\\ 
  SVM	& PCA	& UKOB	&	$54$\\
  DT	& MAN	& UCI	&	$78$ \\ 
  DT	& PCA	& UCI	&	$92$ \\ 
  SVM	& MAN	& UCI	&	$85$ \\ 
  SVM	& PCA	& UCI	&	$64$ \\ \hline
\end{tabular}
\end{center}

Figure \ref{fig:har_eval} shows a plot of these accuracy values.

\begin{figure}[ht]
  \centering
  \includegraphics[width= 0.5 \textwidth]{img/har/accuracy_plot.png}
  \caption{Classification accuracy of HAR classifiers}
  \label{fig:har_eval}
\end{figure}


\pagebreak
\todo{HH}{Expand Discussion of Evaluation Results.}

Upshot:
\begin{itemize}
\item  Decision tree on manual features has acceptable performance
\item  Best classification with SVM on manual features
\item  Still below state of the art classifier by \cite{Anguita} with 97\% accuracy.
\item  Annomalies in Confusion Matrices: Sitting is not detected in UKOB dataset.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../D1-2"
%%% End:
