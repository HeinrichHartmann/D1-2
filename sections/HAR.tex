%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Human Activity Recognition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:HAR}

In this section we describe the implementation of the human acitivity
recognition ('HAR') component on the mobile device.  The algorithmic
foundation of this data mining task have been described in detail in
deliverable D2.2 in section 3.2 ``Mobile Sensing and activity
recognition''. We include a brief summary here (\ref{sec:har_method})
for the sake of completeness. Subsection \ref{sec:har_component}
contains the descriptions of the implementation. In subsection
\ref{sec:har_eval} we evaluate our method to the on two data sets and
compare it to the state of the art approaches.

Despite several attempts to port the component to other mobile
platforms we have not been able to do so, due to technical
difficulties which where hard to anticipate. As described in
deliverable D1.1 it was planned to implement this component using the
Appcelerator Titanium\footnote{\url{http://www.appcelerator.com/}}
framework. Moreover, several viability checks were presented. In the
first prototypes
\footnote{\url{https://github.com/HeinrichHartmann/LiveGovWP1/tree/master/mobile/TitaniumSensorCollector}}
it turned out, that it is not possible to collect sensor data while
running in the background with Titanium. This feature is clearly
required for an integrated activity recognition library which shall be
used in the field trials.

As stated in D1.1 foresees, we propose independent native
implementation as fallback solution in this case. So far we have a
running implementation for Android and Blackberry. An iPhone port is
planned, once the implementation is more stable.


\subsection{HAR Method Summary}\label{sec:har_method}

The process of activity recognition uses a pipeline of signal
processing and machine learning techniques. It consists of two phases:
the ``training phase'' and the ``integration phase''. 

\begin{figure}[htbp]
\centering
\subfigure[HAR Training Phase]{
\label{fig:har_overview}
\includegraphics[width=0.5 \textwidth]{img/har/classification_overview.png}
}
\subfigure[HAR Integration Phase]{
\label{fig:integrated_har_overview}
\includegraphics[width=0.25 \textwidth]{img/har/integration_overview.png}
}
\caption{Overview Human Activity Recognition}
\end{figure}


In the training phase (cf. Figure \ref{fig:har_overview}) a group of
volunteers is asked to perform the targeted activities for a certain
amount of time, while recording sensor samples with the device in
their pocket.  The gained training data stored in a database and used
to train a classifier of the activities.

In the integration phase (cf. \ref{fig:integrated_har_overview}), the
trained classifier is embedded into the mobile device.

Both phases rely on the preprocessing steps of "windowing'' and
``feature generation''. The stream of incoming sensor data is divided into
time windows of fixed size (typically 1-10 sec.) and for each window
a set of features is computed. This features are filtering out
relevant information from the raw signal. Typical features include
mean values and standard deviations as well as frequency domain
features like Fourier modes.

Deliverable D2.2 contains detailed lists of all sensors and features
and data mining methods that are used in the literature as well as
discussions of quality of the recognition results.

\subsection{Component Description}\label{sec:har_component}

Architecture Description.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{img/har/classification_architecture.jpg}
\caption{Classification Architecture}\label{fig:classification_architecture}
\end{figure}

\begin{itemize}
  \item \texttt{SAMPLE PRODUCER} The samples chosen to be used to train the 
    classifier are present in a CSV file. Because of reusability the file has 
    to be imported line by line so it is possible to reuse the whole streaming
    pipeline, which is already in place for the mobile device.
  \item \texttt{WINDOW PRODUCER} Since the classification of the current 
    activity uses a timeframe, the window producer groups them together. It
    allows different window sizes and overlaps.
  \item \texttt{INTERPOLATION \& QUALITY CHECK} To remove frequency disparity 
    and ignore timeframes below a set sample frequency a quality check is in 
    place. After the check, all windows get standardized to a constant frequency
    using interpolation between the data points.
  \item \texttt{FEATURE CALCULATION} The classification runs on different 
    features extracted from the windows (cf. \ref{sec:har_features}).
  \item \texttt{CSV-PERSISTOR} For further processing of the calculated data
    it is required to be store. This is done using a simple CSV data format,
    to ensure cross program importability.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{img/har/integration.jpg}
\caption{Integrated HAR Architecture}\label{fig:integrated_har}
\end{figure}

\begin{itemize}
  \item \texttt{SENSOR EVENT THREAD} The sensor collector is connected to nearly
    all sensors available on the device. The samples get pushed into a queue to 
    keep the calculation time needed for each sensor event at a minimum.
  \item \texttt{DISPATCHER THREAD} This thread distributes the sensor samples to
    each connected component like the persistor or the human activity 
    recognition.
  \item \texttt{HAR PIPELINE} The pipeline reuses most parts described in 
    \ref{fig:classification_architecture}. The difference here is the fully 
    trained classifier, which tries to identify the current activity.
  \item \texttt{INTENT BROADCASTER} After the classifier recognised an activity,
    it gets broadcasted using an intent.
\end{itemize}


\subsection{Features}\label{sec:har_features}

\begin{itemize}
  \item Mean of each individual axis
  \item Variance of each individual axis
  \item Mean of standard deviation
  \item Variance of standard deviation
  \item Bin distribution of standard deviation
  \item Bin distribution of frequency domain
  \item Device tilting angle
  \item Energy
  \item Kurtosis
\end{itemize}


\subsection{Classifier Training}\label{sec:har_classifier_training}

The first step is separating the data set into a train and test set. Here we 
used data from different users for each activity. After both data sets got 
processed by the classification architecture (cf. Figure \ref{fig:classification_architecture})
the training and test set get imported into {\it Weka}\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}}.
Weka comes with build in machine learning algorithms to choose from. Here the 
J48 implementation of the C4.5 algorithm is used to create a decision tree out 
of the provided training set. The test set is used to validate the created tree
and calculate the accuracy and confusion matrix. Since Weka allows to export 
the created tree as Java class, it is possible to integrate the output directly 
into the HAR Pipeline.

% TODO CE
% Describe the component implementation.
% This can be done in a similar fashion like the sensor collector
% further up: Add Bullet point list for the individual components.
% And describe them in one or two sentences.
% * Add a list for the features we use in the current implementation
% * Describe how we train the decision tree classifier using WEKA.
%   Add a screenshot of the results. 
% * Describe how we integrate the classifier into the application

% TODO:
% * Describe Dection Tree Classifier [HH]
% * Describe SVM [CERTH]

\subsubsection*{SVM based classifier}

A popular algorithm to train a binary classification model for mapping
features to activities is the Support Vector Machines (SVMs). SVMs are
known for their ability in smoothly generalizing and coping
efficiently with high-dimensionality pattern recognition
problems. They define a hypothesis space that includes all the
possible linear separations of the data
(Fig.~\ref{fig:svm_hypothesisSpace}) and they choose the one that
maximizes the margin between the two classes
(Fig.~\ref{fig:svm_margin}).

\begin{figure}[h]
\centering
  \includegraphics[width=0.5\textwidth]{img/svms/Svms_hypothesisSpace.pdf}
  \caption{Left: Hypothesis space including all linear separations. Right: The selected hypothesis maximizes the margin.}
  \label{fig:svm_hypothesisSpace}
\end{figure}


\begin{figure}[h]
\centering
  \includegraphics[width=0.5\textwidth]{img/svms/Svm_Maxmargin.pdf}
  \caption{Maximization of the margin}
  \label{fig:svm_margin}
\end{figure}

\noindent The hyperplane that optimally separates the positive and the
negative class (i.e. maximizes the margin) can be described by
$\mathbf{w\cdot x + b = 0}$, where $\mathbf{w}$ is normal to the
hyperplane and $\frac{b}{\parallel\mathbf{w}\parallel}$ is the
perpendicular distance from the hyperplane to the origin
(Fig.~\ref{fig:svm_wb}). The optimal hyperplane can be obtained by
solving the following Quadratic Programming optimization problem:

\begin{equation}\label{Eq:svmQP0}
  \min \frac{1}{2}\parallel\mathbf{w}\parallel \quad \text{s.t.} \quad y_i(\mathbf{w}\cdot x_i + b) -1 \ge 0 \quad \forall i
\end{equation}

In order to relax the constraints of Eq.~\ref{Eq:svmQP0} and allow for
some misclassified points, a slack variable $\xi_i, i=1,\ldots,L$ is
introduced which transforms Eq.~\ref{Eq:svmQP0} into:

\begin{equation}\label{Eq:svmQPxi}
  \min \frac{1}{2}\parallel\mathbf{w}\parallel + C\sum_{i=1}^{L}\xi_i \quad \text{s.t.} \quad y_i(\mathbf{w}\cdot x_i + b) -1 +\xi_i \ge 0 \quad \forall i
\end{equation}

\noindent where the parameter $C$ controls the trade-off between the
slack variable penalty and the size of the margin. In the testing
phase, in order to classify an unseen example $x_t$, its distance to
the hyperplane is calculated using the formula $\mathbf{w}\cdot x_t +
b$. This distance indicates the classifier's confidence that the
unseen example $x_t$ belongs to the examined class.

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svms_wb.pdf}
  \caption{The resulting hyperplane after training an SVM.}
  \label{fig:svm_wb}
\end{figure}

The previous consider linear separation of the data. However, this is
rarely the case for most real world scenarios and for this reason the
kernel trick has been introduced. Applying the kernel trick to the
cases where the classes are not linearly separable in the input
feature space, we manage to map the features to a higher dimension
(Fig.~\ref{fig:svm_highDim}) where they can be linearly separated. For
example, in the case of an RBF kernel the otherwise linear hyperplane
is transformed to a hypersphere (Fig.~\ref{fig:svm_rbf}). For any
kernel $K(x,y)$, the classification model can be represented by a
vector $\textbf{w}$ (i.e. the model parameters), a bias scalar $b$ and
the support vectors $\mathbf{SV}_j, j=1,\ldots,N_{SV}$. For an unseen
example $x_t$, a confidence score is extracted by computing its
distance to the hyperplane of the model:

\begin{equation}\label{Eq:SVMdecision}
  confidence = \mathbf{w}*\sum_{j=1}^{N_{SV}}{K(\mathbf{SV}_j,\mathbf{x}_t)}+b
\end{equation}

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svm_DimensionMap.pdf}
  \caption{The data are not separable in the input space by a linear hyperplane. Using the kernel function, the data are mapped into a higher dimensional space where they can be linearly separated.}
  \label{fig:svm_highDim}
\end{figure}

\begin{figure}[h]
\centering
  \includegraphics[width=0.6\textwidth]{img/svms/Svm_RBF.pdf}
  \caption{Non-linear separation of the data using the RBF kernel}
  \label{fig:svm_rbf}
\end{figure}


In our case, an SVM model is trained on the previously extracted
features in order to learn the properties that define the examined
activity. The models are trained using the one versus all (OVA)
technique, i.e. all positive examples of the specific activity versus
all negative examples (i.e. the examples of all other activities). The
distance of a vector from the hyperplane indicates our confidence that
during the analysed window, the user performs the examined
activity. High positive values of this score increase our confidence
that this window belongs to the positive class while high negative
values provide strong confidence that the performed activity is not
the examined one.

% The fact that each model can be represented by a single vector and a
% scalar and that the testing process is essentially a vector
% multiplication, renders SVMs the best solution for a real time
% classification framework. This allows for storing the information
% about all the models in the phone memory, while testing is
% computationally very efficient, making it possible for the image
% classification algorithm to run entirely on a mobile phone.

\subsection{Evaluation}\label{sec:har_eval}

We have evaluated our classifier on two different datasets.

The first dataset was gathered on the University Campus in Koblenz in
December 2013.  It contains a total of around $900K$ samples collected
by $10$ volunteers.  The volunteers were instructed to perform the
activities ''walking'', ''running'', ''stairs'' and ''cycling'' on
predefined routes on the university campus (cf. Figure
\ref{fig:data_collection_handout}). The total time effort per
volunteer was about 20-25minutes and a financial reward was offered as
an incentive. After the recording the samples have been inspected
using our inspection tool and the beginning and ending of the
activities were manually stripped in order to avoid noise from holding
the device in the hand.

%
% [TODO: Publication of Dataset!]
%

The other dataset was obtained from the {\it UCI Machine Learning
  Repository}\footnote{\url{http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones}}
and was gathered by Davide Anguita, et. al. \cite{Anguita} in 2012.
It contains around $700K$ collected by 30 volunteers.

The number of samples per activity of both datasets are summarized in
Figure \ref{fig:har_datasets}. Both datasets contain only
accelerometer samples, and have been preprocessed that have been
sampled at a fixed rate of $50Hz$.

\begin{figure}
\centering
\begin{tabular}{|l|r|r|} \hline
Activity  & UCI Dataset & UKOB Dataset \\ \hline
sitting   & 113.728     & 80.951        \\
standing  & 121.984     & 320.737       \\
walking   & 110.208     & 292.024       \\
running   & 0           & 31.916        \\
cycling   & 0           & 436.106       \\
stairs    & 188.800     & 30.086        \\
lying     & 124.416     & 0            \\ \hline \hline
Totals    & 659.136     & 903.156       \\ \hline
\end{tabular}

\caption{Number of accelerometer samples by activity and dataset.}
\label{fig:har_datasets}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{img/har/data_collection_handout.png}
  \caption{Instructions for data collection in German
    language}\label{fig:data_collection_handout}
\end{figure}


%
% TODO: Describe Evaluation Results
%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../D1-2"
%%% End:
